\documentclass[twocolumn]{revtex4-1}

% preamble
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{physics}
\usepackage[mathscr]{euscript}

\raggedbottom
\newcommand{\ud}{\, \text{d}}
\newcommand{\code}[1]{\textcolor{blue}{\lstinline{#1}}}
\newcommand{\bld}[1]{\vectorbold{#1}}
\newcommand{\cC}{\mathscr{C}}
\newcommand{\parpar}[2]{\frac{\partial #1}{\partial #2}}

\begin{document}

\title{Vectorized Neural Network Implementation With Back-Propagation}
\author{Keith D. Matthews}
% \affiliation{Acerbic Associates}

\date{1 September 2022}

\begin{abstract}
In the pursuit of a thorough understanding of the basic principles behind the back-propagation algorithm I thought I'd better construct a neural network simulation library from scratch. This document describes the design of that library.
\end{abstract}

\maketitle

% ======================================

\section{Notation and Eqns of Forward-Propagation}

Interaction with the simulated network is through the \code{Network} class. Underlying the \code{Network} is a  \code{Layer} class (but no node class behind that.) Forward propagation is triggered with a call to \code{network.outputs()} which in turn calls \code{layer.outputs()} which in turn calls the previous layer's \code{previous_layer.outputs()} and on and on until there remains no previous layer.

The pre-activation output (AKA the 'coalesced input') of node $m$ on layer $l$ is defined by
\begin{align}
    \label{eqn:componentCoalescence}
    z^l_n = W^l_{p n} \, a^{l - 1}_p + b^{l}_n
\end{align}

The $a^{l - 1}_p$ are the post-activation outputs of the immediately upstream (prior) layer, $l-1$. $W^l_{p n}$ are the weights connecting those outputs of layer $l - 1$ to the inputs of layer $l$. (Note that $W^l_{p n}$ is associated with layer $l$ and not with layer $l-1$.) These weights are expressed as a matrix, $W^l_{b a}$ where $b$ indexes the previous layer's nodes, while $a$ indexes the current layer's nodes. Similarly $b^l_n$ are the biases for each of the nodes in layer $l$ and is well expressed as a vector with one component for each node in layer $l$.

The input layer, where $l = 0$, is denoted $\bld{L}^0$ and is special in that it has no weights or biases associated with it.

The reader will understand that the expression of each of these quantities in abstract/bold-face notation, e.g. $\bld{a}^l$, refers to the same object as the index notation version, $a^l_n$. Thus we may express the relationship between the nodes' activation outputs, $\bld{a}^l$ and their coalesced inputs, $\bld{z}^l$, as 
\begin{align}
    \label{eqn:activation}
    \bld{a}^l = g(\bld{z}^l)
\end{align}
where $g$ is called the activation function. Equivalently, in component notation we have
\begin{align}
    \label{eqn:componentActivation}
    a^l_n = g(z^l_n)
\end{align}
where Eqns. (\ref{eqn:activation}) and (\ref{eqn:componentActivation}) as equivalent.

Together eqns (\ref{eqn:componentCoalescence}) and (\ref{eqn:componentActivation}) can be used to chain the computation from layer to layer until computation of the network's output is complete. Furthermore, by taking derivatives of these, along with a cost function, we will derive back-propagation.

Please note that $g$ does not mix components the way $\bld{W}$ does, instead it acts like a scalar treating each of the components of its input independently. 

\section{Training Data}
The \textit{predictions} (network level outputs) of the network are denoted $\bld{\hat{y}}$ while the inputs (\textit{examples}) are denoted $\bld{x}$. Training data is composed of examples labeled with known ground truth outputs denoted $\bld{y}$. We identify the predictions $\hat{y}_n$ with the activation values $a^f_n = \hat{y}_n$ when the inputs are $z^0_p = x_p$ set to the example values. With those definitions we may define the cross-entropy cost function, denoted $\mathscr{C}$, that scores the predictions according to how closely they match the ground truth.

\begin{align}
    \label{eqn:costDefn}
    \cC \equiv - \sum_n \left(y_n \ln \hat{y}_n \right)
\end{align}
The idea is that the greater the gap between the predicted outputs and ground truth the higher the score. By adjusting the weights and biases we hope to minimize the cost and thereby bring the predicted outputs in alignment with the ground truth labels. This process is known as \textit{training} the network. Note that the summation is gone, it has been dissolved by the derivative which picked out just one term.

We will also need the cost function's derivative which we derive analytically.
\begin{align}
    \label{eqn:derivCost}
    \parpar{\cC}{\hat{y}_r} = - \frac{y_r}{\hat{y}_r}
\end{align}

There are myriad cost functions and we're told that some work better in some circumstances. For this document we're only going to consider the cross-entropy cost.

\subsection{Testing Vs. Training Data}
Most researches will take a portion of their data set and set it aside to insure that it will not be used for training the model. This data is then available to test how well the newly trained model performs on data it has not seen before. This set aside is known as a \textit{test dataset}. The term \textit{training data} refers to the majority of the original data set that remains available to train the model via back-propagation.

\section{Back-Propagation}
Now we have all the pieces in place to discuss back-propagation, an algorithm for refining ours weights and biases to iteratively reduce the overall cost associated with our training set. 

\begin{align}
    \label{eqn:deltas}
    \Delta \bld{W} & \approx - \alpha \parpar{\cC}{\bld{W}} \\
    %
    \Delta \bld{b} & \approx - \alpha \parpar{\cC}{\bld{b}}
\end{align}
We may imagine these equations as the first order terms in Taylor expansions of the Cost function in terms of our weights and biases, and interpret our efforts as computing first order approximations to the platonic $\Delta$'s. The partial derivatives on the right can be expanded using the chain rule. We may interpret this expansion as our proceeding progressively backwards through the network layer-by-layer. We start with outputs of the output layer itself, differentiate with respect to (wrt) the output layer's inputs, then cross to the prior layer's outputs and so on.

Lets consider only the output layer, layer $f$, and evaluate our approximations to $\Delta \bld{W}^f$ and $\Delta \bld{b}^f$ in the hopes that this simplest layer will be instructive. We have
\begin{align}
    \parpar{\cC}{W^f_{p n}} = \parpar{\cC}{a^f_n} \parpar{a^f_n}{z^f_n} \parpar{z^f_n}{W^f_{p n}}
\end{align}
Note that we're playing a bit fast and loose with the notation. The first term is a simple vector. But you could well argue that we've been a little dishonest in the second term by assuming that the node index on both top and bottom should be the same. In general that is certainly not true, but it in this case we'll be able to support the claim when we expand the term. However, our high-handedness does require some care in implementation. We have smashed this second term, which should have been a diagonal matrix, down to a vector and now the first two terms are not connected by the dot product one might have expected, but instead by element-wise multiplication. On the plus side we have saved a lot of wasteful flops in the process. 

Similarly, the repeated $n$-index in the third term might bother you, and indeed in this  the $n$ will drop out completely and we'll be left with a term that depends solely on $n$ which we can only connect with the first two terms, with which no index is shared, with an outer product.


\subsection{Output Layer Delta Weights}
The first term is just a matter of grinding it out.
\begin{align}
    \label{eqn:costWRTaF}
    \parpar{\cC}{a^f_n} = - \frac{y}{a^f_n}
\end{align}
The results are useful, but clunky. We'll just refer to this term as $\parpar{\cC}{a^f_n}$ and say no more about it. The second term is more interesting. It is obtained from the general layer $l$ expression
\begin{align}
    \label{eqn:aWRTzL}
    \parpar{a^l_n}{z^l_n} = g'(z^l_n)
\end{align}
by subtituting $l \rightarrow f$. If it weren't for the scalar behavior of the activation function $g$ we wouldn't be able to get away with expressing this object as a vector with the same index appearing on both top and bottom of the derivative symbol.

We will refer to the third term as the \textit{weights suffix} for reasons which we hope will become clear in a moment.
\begin{align}
    \label{eqn:zWRTweightsL}
    \parpar{z^l_n}{W^l_{p n}} = a^{l-1}_p
\end{align}
As above we must take this general layer-$l$ expression and subtitute $l \rightarrow f$ in reference to the final layer. Note how the $n$ index doesn't appear in the RHS. In general this term would have been a rank 3 object (one with 3 indices) except that we are contracting (summing over) 2 of these indices leaving us with a 1 index object, a vector.

Putting those three, (\ref{eqn:costWRTaF}), (\ref{eqn:aWRTzF}), (\ref{eqn:zWRTweightsF}), together, along with the appropriate forms of array multiplication, we have the core of the output layer's delta weights.
\begin{align}
    \label{eqn:costWRTweightsF}
    \parpar{\cC}{W^f_{p n}} = \left(\parpar{\cC}{a^f_n} \times g'(z^f_n) \right) \otimes a^{f-1}{p}
\end{align}
We use the $\times$ operator to denote element-wise multiplication, and the $\otimes$ operator to denote an outer product.


\subsection{Output Layer Delta Biases}
Now that we understand the crucial part of the $\Delta \bld{W}^f$ we would like to extend that understanding to the equivalent formula for the final layer biases, $\Delta \bld{b}^f$. The baises will turn out to be slightly simpler than the weights.
\begin{align}
    \label{eqn:costWRTbiasesF}
    \parpar{\cC}{b^f_n} = \parpar{\cC}{a^f_n} \parpar{a^f_n}{z^f_n} \parpar{z^f_n}{b^f_n}
\end{align}
where we note that 
\begin{align*}
    \parpar{z^f_n}{b^f_n} = 1_n
\end{align*}
leaving us with
\begin{align}
    \parpar{\cC}{b^f_m} = \parpar{\cC}{a^f_m} \times g'(z^f_m)
\end{align}
where we have dropped the unnecessary vector of 1s. 
Having dropped the third term entirely we wind up with an expression similar to but simpler than that for the weights, Eqn. (\ref{eqn:costWRTweightsF}). (Note how the denominator changes from $W^f_{m n}$ to $b^f_m$ as we switch from calculating $\Delta \bld{W}$ to calculating $\Delta \bld{b}$.)

We refer to the third term that remains in delta weights, but is absent from delta biases, as the \textit{weights suffix}  (\ref{eqn:zWRTweightsF}); it is the additional term that converts the delta biases into the delta weights. This is true for every layer. Our procedure will be to compute the delta biases first, and then apply the weights suffix to construct the delta weights for the same layer, thereby saving a lot of wasted computation.

\subsection{Layer $f-1$}
Let's examine the same derivation extended slightly to address the last interior layer, layer $f-1$. You'll note that the full formula for the $f-1$ layer delta weights is very similar to that for layer $f$. We make a tiny modification to the weights suffix, and we add two new, but very familiar, terms in the middle.

As before we simply walk up through the layers, from output to input and to the prior layer's output.
\begin{align}
    \parpar{\cC}{W^{f-1}_{q p}} = & \parpar{\cC}{a^f_n} \parpar{a^f_n}{z^f_n} & \textit{start monomer}\nonumber \\
    & \cdot \left(\parpar{z^f_n}{a^{f-1}_p} \parpar{a^{f-1}_p}{z^{f-1}_p} \right) & \textit{monomer}\nonumber \\
    & \cdot \parpar{z^{f-1}_p}{W^{f-1}_{q p}} & \textit{weights suffix}
\end{align}
A chemist would tell us that a chain of monomers makes a polymer. By analogy, sequences of our monomers polymerize into a chainlike formula for delta weights and delta biases.

\subsection{Establishing The Pattern On Layer $f-2$}
Just for fun let's jump right to $f-2$ just to see if we can identify the pattern that makes this problem tractable.
\begin{align}
    \parpar{\cC}{W^{f-2}_{r q}} = & \parpar{\cC}{a^f_n} \parpar{a^f_n}{z^f_n} & \textit{start monomer} \nonumber \\
        & \cdot \left(\parpar{z^f_n}{a^{f-1}_p} \parpar{a^{f-1}_p}{z^{f-1}_p} \right) & \textit{monomer $(f-1)$} \nonumber \\
        & \cdot \left(\parpar{z^{f-1}_p}{a^{f-2}_q} \parpar{a^{f-2}_q}{z^{f-2}_q} \right) & \textit{monomer $(f-2)$} \nonumber \\
        & \parpar{z^{f-2}_q}{W^{f-2}_{r q}} & \textit{weights suffix}
\end{align}

The key realization here is that we can calculate the delta weights and delta biases for any layer in any neural network just by inserting as many factors of the monomer as required to reach the target layer.

We've already evaluated the \textit{start monomer} (the first two terms in eqn. (\ref{eqn:costWRTweightsF}))
\begin{align}
    \label{eqn:startMonomer}
    \parpar{\cC}{a^f_n} \parpar{a^f_n}{z^f_n} = \parpar{\cC}{a^f_n} \times g'(z^f_n)
\end{align}
and the \textit{weights suffix}, eqn. (\ref{eqn:zWRTweightsL}). That just leaves us to evaluate the monomer, starting at layer $f-1$.
\begin{align}
    \label{eqn:monomerAtFm1}
    \parpar{z^f_n}{a^{f-1}_p} \parpar{a^{f-1}_p}{z^{f-1}_p} = W^f_{p n} \times g'(z^{f-1}_p)
\end{align}
Extending the monomer pattern to layer $f-2$ we have
\begin{align}
    \label{eqn:monomerAtFm2}
    \parpar{z^{f-1}_p}{a^{f-2}_q} \parpar{a^{f-2}_q}{z^{f-2}_q} = W^{f-1}_{q p} \times g'(z^{f-2}_q)
\end{align}
In general we have 
\begin{align}
    \label{eqn:monomerForTargetL}
    \parpar{z^{l+1}_n}{a^l_p} \parpar{a^l_p}{z^l_p} = W^{l+1}_{p n} \times g'(z^l_p)
\end{align}
which is a matrix. Were we to choose to target layer $f-3$ we anticate that we would have to insert three of these monomers, one targeting layer $f-1$, the next targeting layer $f-2$ and the third targeting layer $f-3$. Not only that, but the delta biases for the subsequent layer provides the starting point for this layer. Between any two adjacent layers we need only perform a single matrix multiplication.

The general formula for delta biases for arbitrary layer $l$ is
\begin{align}
    \label{eqn:deltaBiases}
    \Delta \bld{b}^l = -\alpha {\left( \parpar{\cC}{\bld{a}^f} \times g'(\bld{z}^f) \right) \
        \prod_{k = f-1}^l {\left( \bld{W}^k \times g'(\bld{z}^k) \right)}^T}
\end{align}
where, unless denoted $\times$ or $\otimes$, multiplication is standard matrix multiplication. The delta weights is very similar.
\begin{align}
    \label{eqn:deltaWeights}
    \Delta \bld{W}^l = - \alpha {\left( \left( \parpar{\cC}{\bld{a}^f} \times g'(\bld{z}^f) \right) \
        \prod_{k = f-1}^l {\left( \bld{W}^k \times g'(\bld{z}^k) \right)}^T \otimes \bld{a}^{l-1} \right)}^T
\end{align}
These are the two equations required for back-propagation. 

\section{Implementation Notes}
We have expressed eqns. (\ref{eqn:deltaBiases}) and (\ref{eqn:deltaWeights}) in terms of matrix operations rather than in our preferred component notation because the implementation is simpler.

Note that we made a poor choice for the indexing of $W^l_{b a}$. The motivation was that the first index, in this case $b$, would enumerate the prior layer's nodes and the second index, $a$, this layer's nodes. At the time this seemed reasonable enough. Unfortunately it winds up requiring each monomer to be transposed before it can be applied. It would have been cleaner to have chosen $W^l_{a b}$.

\section{Vectorization}
Having had the opportunity to train a couple neural networks based on toy model data available through sci-kit learn we discovered the value of being able to compute the output for an arbitrary number of examples with a single function call. This can be accomplished by taking what was previously a vector (rank 1 object) of features and expand it to become a matrix (rank 2 object) where the first index still counts over features, but we also introduce a new second index that counts over the multiple examples. This enormously reduces the computational expense of computing the cost for an entire data set by taking advantage of clever libraries and specialized hardware that perform matrix operations extraordinarily quickly.

\subsection{Numpy's Einsum}
I have experience using Einstein summation notation for describing operations on matrix-like objects and so I am naturally drawn to Numpy's \code{einsum()} routine. It unifies a wide range of array operations within a single function call. Rather than mucking about with transposes and axis-specific summations and element-wise matrix multiplication, I ultimately decided to transform all that complexity into fashioning cryptic little \code{einsum()} directive strings like $'a, ba, cb, dc \rightarrow bd'$ which expresses a funny sort of multiplication of four input arrays (rank 1, 2, 2, 2 respectively) and produces a single output. That output has rank-2 because but dummy index, $b$, triggered multiplication but not summation. It was not contracted upon because of its appearance on the RHS of the directive.

Einsum is also quite fast, and I haven't even explored the option to 'compile' einsum directives yet.

It's not that it's impossible to express the desired operations in standard matrix operations. Consider the exceedingly common operation of multiplying a weights matrix against the previous layer's outputs: $W^l_{p n} \, a^{l - 1}_p$. This can be expressed in standard array operations as \code{np.dot(W.T, a)} or equivalently with the Einsum directive $'pn,p \rightarrow n'$ where we are contracting over $p$. There is no particular advantage to either approach in this case.

Next we'll introduce an additional index that counts over all the examples. We are then faced with $W^l_{p n} \, a^{l - 1}_{p m}$. It is well represented by the exact same expression, \code{np.dot(W.T, a)}, while in Einsum it becomes $pn,pm \rightarrow nm$. 

Now let's do hard example, let's implement Eqn. (\ref{eqn:costWRTweightsF}). We start with the element-wise multiplication of the first two terms. $t1$ is the cost deriv term and has the shape $(N_n, N_m)$ where $N_n$ is the number of nodes on layer $l$ and $N_m$ is the number of examples. $t2$ is the deriv-a-wrt-z term and has the same shape $(N_n, N_m)$. Suppose you try to perform the multiplication with \code{np.multiply(t1, t2)}. Are you sure what numpy is going to do? What if $N_m$ just so happens to be broadcastable to $N_n$? Fairly frequently $N_m = 1$, which is broadcastable to anything.

With Einsum you won't have this problem. The directive is very specific about how each index is to be treated and in what order. If you want the $m$-index to be treated differently than the others, that's no problem. The whole formula can be expressed as $'nm,nm,pm -> pnm'$ leaving no ambiguity as to the desired operation. 

It was my intention, for better or worse, that the $m$-index always be the last one. This probably isn't the best choice when examining debugging output, but it seemed simple at the time. It is not trivial to achieve such a goal with standard matrix multiplication operations where the order of the indices is dictated by the operation, and not by the programmer.

\end{document}